# Part3 梯度下降

### 1. 什么是梯度下降

对于假设函数，我们需要一种方法来衡量它与数据之间契合程度，那么就需要对假设函数中的θ参数评估，这种方法就是梯度下降。

当我们进行衡量假设函数与数据之间的契合程度时，参考的标准是代价函数的取值。
$$
在前文的例子中，我们把\theta_0作为x轴，\theta_1作为y轴，代价函数作为z轴，绘制的图像如下所示：
$$
![gradientDescent01](/img/gradientDescent01.png)

使代价函数取得最小值是我们的任务，所以在给定两个θ的初值之后，不管我们此时处于图像上的哪一点，我们都想找到一个方向从山上以最快的速度下降，然后到达一个局部最底点，值得注意的是，给定不同的初始值，我们最终可能到达不同区域的最低点。如图所示，红色箭头标注的是最小点，可以从不同的起始位置达到。

那么梯度下降时采用的方法是采用代价函数的导数，该点处的导数即是切线的斜率，在每一个点处求导，并沿着代价函数的切线方向最陡下降，迭代直至收敛，每一步的大小由学习率α决定。
$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0, \theta_1)\\
其中，j=0, 1，代表的是特征变量的序号
$$
在每一次迭代中，需要同步计算好所有的变量θ的值之后，再去同步更新，否则会导致错误。

![](/img/gradientDescent02.png)